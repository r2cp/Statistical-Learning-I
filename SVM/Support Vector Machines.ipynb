{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "## ¿Qué son las SVM?\n",
    "\n",
    "Support Vector machine (SVM) es una herramienta de aprendizaje muy potente y versátil. Modelo, capaz de realizar clasificación lineal, no lineal, regresiones e incluso detección de anomalías. Originariamente se desarrolló como un método de clasificación binaria, su aplicación se ha extendido a problemas de clasificación múltiple y regresión. Los SVMs son particularmente útiles para la clasificación de conjuntos de datos complejos pero de tamaño pequeño o mediano.\n",
    "\n",
    "Las SVM se fundamentan en el *Maximal Margin Classifier*, que a su vez, se basa en el concepto de hiperplano.\n",
    "\n",
    "### Hiperplano\n",
    "\n",
    "En un espacio $p$-dimensional, un hiperplano se define como un subespacio plano y afín de dimensiones $p−1$. El término afín significa que el subespacio no tiene por qué pasar por el origen. En un espacio de dos dimensiones, el hiperplano es un subespacio de 1 dimensión, es decir, una recta. En un espacio tridimensional, un hiperplano es un subespacio de dos dimensiones, un plano convencional. Para dimensiones $p>3$ no es intuitivo visualizar un hiperplano, pero el concepto de subespacio con $p−1$ dimensiones se mantiene. Cada segmento del hiperplano representa un segmento de la clasificación que se tiene, como se puede ver en la siguiente imagen:\n",
    "\n",
    "<img src=\"images/svm1.png\" style=\"width: 400px;\">\n",
    "\n",
    "La imagen muestra el hiperplano de un espacio bidimensional. La ecuación que describe el hiperplano, que es una recta, tiene la forma $1+2x+3y=0$. La región azul representa el espacio en el que se encuentran todos los puntos para los que $1+2x+3y>0$ y la región roja el de los puntos para los que $1+2x+3y<0$. Teniendo para estos casos dos segmentos con nuestros datos. \n",
    "\n",
    "\n",
    "Cuando se dispone de n observaciones, cada una con p predictores y cuya variable respuesta tiene dos niveles (de aquí en adelante identificados como $+1$ y $−1$), se pueden emplear hiperplanos para construir un clasificador que permita predecir a que grupo pertenece una observación en función de sus predictores. Es decir, regresando al ejemplo, que observaciones corresponden a cada uno de los lados de la recta, clasficando a cada punto como rojo o azul.\n",
    "\n",
    "Si la distribución de las observaciones es tal que se pueden separar linealmente de forma perfecta en las dos clases ($+1$ y $−1$), entonces, un hiperplano de separación cumple que:\n",
    "\n",
    "$$ \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_p x_p + b \\geq 0 \\quad \\text{si} \\quad y_i = 1$$\n",
    "\n",
    "$$ \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_p x_p + b < 0 \\quad \\text{si} \\quad y_i = 0$$\n",
    "\n",
    "Al identificar cada clase como $+1$ o $−1$, y dado que multiplicar dos valores negativos resultan en un valor positivo, las dos condiciones anteriores pueden simplificarse en una única:\n",
    "\n",
    "$$ y_i (\\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_p x_p + b) > 0, \\quad \\text{para} \\quad i= 1, \\ldots, n $$\n",
    "\n",
    "Bajo este escenario, el clasificador más sencillo consiste en asignar cada observación a una clase dependiendo del lado del hiperplano en el que se encuentre. Es decir, la observación $x^*$ se clasifica acorde al signo de la función \n",
    "\n",
    "$$f(x^*) = \\theta_1 x^*_1 + \\theta_2 x^*_2 + \\ldots + \\theta_p x^*_p + b$$.\n",
    "\n",
    "Si $f(x^*) \\geq 0$, entonces la observación se asigna a la clase $+1$; si por el contrario, $f(x^*) < 0$, entonces se asigna a la clase $−1$. Además, la magnitud de $f(x^*)$ permite saber cómo de lejos está la observación del hiperplano y con ello la confianza de la clasificación.\n",
    "\n",
    "La definición de hiperplano para casos perfectamente separables linealmente resulta en un número infinito de posibles hiperplanos, lo que hace necesario un método que permita seleccionar uno de ellos como clasificador óptimo.\n",
    "\n",
    "La solución a este problema consiste en seleccionar como clasificador óptimo al que se conoce como maximal margin hyperplane o hiperplano óptimo de separación, que se corresponde con el hiperplano que se encuentra más alejado de todas las observaciones de entrenamiento. Para obtenerlo, se tiene que calcular la distancia perpendicular de cada observación a un determinado hiperplano. La menor de estas distancias (conocida como margen) determina como de alejado está el hiperplano de las observaciones de entrenamiento. El maximal margin hyperplane se define como el hiperplano que consigue un mayor margen, es decir, que la distancia mínima entre el hiperplano y las observaciones es lo más grande posible.\n",
    "\n",
    "<img src=\"images/svm2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Para qué sirven?\n",
    "\n",
    "Constituyen un método basado en aprendizaje para la resolución de problemas de clasificación y regresión. En ambos casos, esta resolución se basa en una primera fase de entrenamiento (donde se les informa con múltiples ejemplos ya resueltos, en forma de pares {problema, solución}) y una segunda fase de uso para la resolución de problemas. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferencias entre SVM y modelos vistos\n",
    "\n",
    "### SVM vs. Modelos de regresión lineal\n",
    "\n",
    "Primeramente lo que ambos tienen en común: son algoritmos usados para la clasificación binaria entre categorías. El objetivo de un clasificador binario es encontrar la línea o curva que tenga la mayor probabilidad de predecir o capacidad de delimitar dos categorías o áreas de datos de manera correcta. Esto se logra al minimizar su función de pérdida\n",
    "\n",
    "La regresión logística trata de encontrar la línea que mejor separe las categorías, los SVM’s tratan de encontrar directamente la mayor separación posible entre las categorías. Luego, comparando las funciones de pérdida, se tiene lo siguiente:\n",
    "\n",
    "<img src=\"images/smv.png\">\n",
    "\n",
    "#### Diferencias de SVM respecto a los modelos de regresión\n",
    "\n",
    "1. La función de pérdida logística se acerca a 0 asintóticamente, pero nunca llega a ser completamente 0 aunque el punto en cuestión sea clasificado de manera suficientemente confiable.\n",
    "2. La función de pérdida logística diverge más rápidamente que la de bisagra, lo cual la hace más sensible a los datos extremos.\n",
    "3. En general, los SVM se desempeñan marginalmente mejor que la regresión logística.\n",
    "4. La función de coste de la regresión logística diverge más rápido que la de las SVM, por lo que será más sensible a la presencia de datos atípicos. Provocando peores resultados cuando existe este tipo de valores en el conjunto de datos.\n",
    "\n",
    "### SVM vs KNN\n",
    "\n",
    "KNN tiene algunas propiedades interesantes: es automáticamente no lineal, puede detectar datos distribuidos lineales o no lineales, tiende a funcionar muy bien con una gran cantidad de puntos de datos. Al igual SVM se puede emplear de forma lineal o no lineal acorde con el uso de un kernel, que esta definido por la cantidad de puntos que se tienen.\n",
    "\n",
    "#### Diferencias de SVM respecto KNN \n",
    "\n",
    "1. En general KNN es muy sensitiva con los datos, por lo que si nuestra serie de datos contiene outliers esto producira clasificaciones erroneas, mientras que en SVM no es relevante el tema de outliers ya que clasifica donde esta el grueso de información.\n",
    "2. Para muchos puntos en poco espacio KNN es mejor opción que SVM.\n",
    "3. Para pocos puntos en mucho espacio SVM es mejor opción que KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas\n",
    "\n",
    "Una de las principales ventajas de utilizar una SVM en lugar de una regresión logística es que las clases podrían no ser linealmente separables. En ese caso, es preferible utilizar una SVM con un núcleo no lineal (por ejemplo el kernel gaussiano), siendo esta una de las mejores opciones en terminos practicos.  Otra de las ventajas del uso SVM es si se trabaja con un espacio de muchas dimensiones. Por ejemplo, se ha encontrado que SVMs funcionan bastante bien para la clasificación de texto.\n",
    "\n",
    "Otra de las ventajas destacables del algoritmo, es que puede realizar clasificaciones de bases de datos desbalanceados, es decir, problemas en los cuales el numero de positivos puede ser pequeño en relación al número de negativos. Así tambien, funciona bien incluso con datos no estructurados y semiestructurados como texto, imágenes y árboles. Por último,  los modelos SVM tienen generalización en la práctica, es decir, el riesgo de sobreajuste es menor en SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desventajas\n",
    "\n",
    "La principal desventaja de SVM es que el proceso de entrenamiento es ineficiente. De tal manera, no suele ser un algoritmo recomendable para una gran cantidad de datos, pues los tiempos de entrenamiento son demasiado largos, es decir, para problemas industriales. Otra dificultad radica en la elección de la función de kernel, pues determinar una \"buena\" función no es fácil.\n",
    "\n",
    "Entre otras dificultades, dado que el modelo final no es tan fácil de ver, no es posible realizar pequeñas calibraciones en el modelo, por lo que es difícil incorporar la lógica del negocio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick\n",
    "\n",
    "Si nuestra información no puede ser trabajada para poder separarse linealmente, como por ejemplo si los datos están dispersos en forma circular, para estos casos que no se puede separar linealmente la información es necesario tener una versión generalizada de SVM que pueda emplearse tanto para formas lineales, circulares, o cualquier forma que se tenga, con la unica finalidad sea encontrar un limite que tome las decisiones optimas que dividan la información en clases.\n",
    "\n",
    "Para entender de mejor manera se definira el siguiente teorema:\n",
    "\n",
    "***\n",
    "### Teorema de Mercer:\n",
    "Dice que si una función $K(a, b)$ satisface todas las restricciones llamadas restricciones de mercer, entonces existe una función que mapea $a$ y $b$ en una dimensión más alta.\n",
    "***\n",
    "\n",
    "Si cumple con la restricciones del teorema de Mercer, es un método que usa un clasificador lineal para determinar puntos de datos no lineales. Matemáticamente, aplicando el teorema de Mercer, que mapea puntos de datos de entrada no lineales en una dimensión más alta donde pueden ser linealmente separables. Y el núcleo es una función que realmente realiza la tarea anterior para nosotros. Existen diferentes tipos de kernel, como \"lineal\", \"polinomial\", \"función de remarcado\", etc. La selección del kernel correcto que mejor se adapte a los datos se obtiene mediante validación cruzada.\n",
    "\n",
    "Un kernel ($K$) es una función que devuelve el resultado del dot product entre dos vectores realizado en un nuevo espacio dimensional distinto al espacio original en el que se encuentran los vectores. Aunque no se ha entrado en detalle en las fórmulas matemáticas empleadas para resolver el problema de optimización, esta contiene un dot product. Si se sustituye este dot product por un kernel, se obtienen directamente los vectores soporte (y el hiperplano) en la dimensión correspondiente al kernel. Ha esto se le suele conocer como kernel trick, porque, con solo una ligera modificación del problema original, gracias a los kernels, se puede obtener el resultado para cualquier dimensión. Existen multitud de kernels distintos, algunos de los más utilizados son:\n",
    "\n",
    "### Kernel lineal \n",
    "\n",
    "$$K(x,x')=x⋅x'$$\n",
    "\n",
    "Si se emplea un Kernel lineal, el clasificador Support Vector Machine obtenido es equivalente al Support Vector Classifier. \n",
    "\n",
    "<img src=\"images/svm3.png\">\n",
    "\n",
    "\n",
    "\n",
    "### Kernel polinómico\n",
    "\n",
    "$$K(x,x')=(x⋅x'+c)d$$\n",
    "\n",
    "Cuando se emplea $d=1$ y $c=0$, el resultado es el mismo que el de un kernel lineal. Si $d>1$, se generan límites de decisión no lineales, aumentando la no linealidad a medida que aumenta d. No suele ser recomendable emplear valores de mayores 5 por problemas de overfitting. \n",
    "\n",
    "<img src=\"images/svm4.png\">\n",
    "\n",
    "## Gaussian Kernel (RBF) \n",
    "\n",
    "<img src=\"images/svm5.png\">\n",
    "\n",
    "$$K(x,x')=\\mathrm{exp}(−\\gamma||x−x'||^2)$$\n",
    "\n",
    "El valor de γ controla el comportamiento del kernel, cuando es muy pequeño, el modelo final es equivalente al obtenido con un kernel lineal, a medida que aumenta su valor, también lo hace la flexibilidad del modelo. \n",
    "\n",
    "Los *kernels* descritos son solo unos pocos de los muchos que existen. Cada uno tiene una serie de hiperparámetros cuyo valor óptimo puede encontrarse mediante validación cruzada. No puede decirse que haya un kernel que supere al resto, depende en gran medida de la naturaleza del problema que se esté tratando. Ahora bien, tal como indican los autores de A Practical Guide to Support Vector Classification, es muy recomendable probar el kernel RBF. Este kernel tiene dos ventajas: que solo tiene dos hiperparámetros que optimizar (γ y la penalización C común a todos los SVM) y que su flexibilidad puede ir desde un clasificador lineal a uno muy complejo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
